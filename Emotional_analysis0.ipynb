{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Emotional Sentiment Analysis and Adaptive Response System\n",
        "\n",
        "In this project, we will build a chatbot capable of identifying and understanding emotional states from text input. The system will analyze user input to detect emotions such as fear, joy, sadness, and more. Based on the detected emotion, the chatbot will generate culturally sensitive and empathetic responses.\n",
        "\n",
        "Steps Involved:\n",
        "\n",
        "1.**Data Collection & Preprocessing**: We will load a dataset containing text data labeled with emotions and preprocess it for model training.\n",
        "\n",
        "2**.Model Training**: Using a machine learning model (e.g., Logistic Regression), we will classify the emotional state of the input text.\n",
        "\n",
        "3.**Response Generation**: After detecting the emotion, the chatbot will generate relevant responses tailored to the emotional context.\n",
        "\n",
        "4.**Integration & Testing**: Finally, we will integrate the sentiment analysis model with the response generation system and test the chatbot’s performance.\n",
        "The goal is to create an empathetic chatbot that understands emotional cues and responds appropriately, providing emotional support."
      ],
      "metadata": {
        "id": "fkwrDIMsnqPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zGWerwai9D-",
        "outputId": "6cffb9c4-4f15-4f20-fabb-adde519ae37c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch pandas nltk spacy openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " EXPLANATION\n"
      ],
      "metadata": {
        "id": "cg5m4HErouLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transformers: A popular library for working with pre-trained models like GPT, BERT, and others for tasks like text generation, sentiment analysis, and more.\n",
        "\n",
        "torch: The core package for PyTorch, a deep learning framework used for building and training neural networks.\n",
        "\n",
        "pandas: A powerful library for data manipulation and analysis, especially useful for handling tabular data (like CSVs).\n",
        "\n",
        "nltk: The Natural Language Toolkit, which provides tools for text processing and analysis, such as tokenization, stopword removal, and more.\n",
        "\n",
        "spacy: Another NLP library that provides advanced text processing tools like tokenization, named entity recognition, and dependency parsing.\n",
        "\n",
        "openai: This library gives you access to OpenAI's models, such as GPT, for various NLP tasks like generating text, answering questions, etc."
      ],
      "metadata": {
        "id": "MikXBY88oipC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will open a dialog to upload the file from your local machine\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "EIg6xpaOkwyx",
        "outputId": "c8003f63-f210-43df-d068-3e8a97c638b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb8d649e-9ea3-4adc-a049-e04cc95c929e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb8d649e-9ea3-4adc-a049-e04cc95c929e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Emotion_classify_Data.csv to Emotion_classify_Data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " EXPLANATION"
      ],
      "metadata": {
        "id": "SLd4_M6Uo5Gr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`from google.colab import files` This imports the files module\n",
        "from Google Colab, which allows you to upload files.\n",
        "\n",
        "`uploaded = files.upload() `This triggers the file upload dialog, allowing you to select files from your local system to upload. Once uploaded, the files are stored in the Colab environment and can be accessed for further processing."
      ],
      "metadata": {
        "id": "F-amtxbupGNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5M4zchmk-6w",
        "outputId": "1ae0b98d-a3cd-4076-8972-3e3f2490f272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'archive.zip', 'Emotion_classify_Data.csv', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Emotion_classify_Data.csv')\n",
        "\n",
        "# Display the first few rows to inspect the data\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng4W4bwzlHhq",
        "outputId": "04561383-41fc-4e64-b083-ba45a7c69938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Comment Emotion\n",
            "0  i seriously hate one subject to death but now ...    fear\n",
            "1                 im so full of life i feel appalled   anger\n",
            "2  i sit here to write i start to dig out my feel...    fear\n",
            "3  ive been really angry with r and i feel like a...     joy\n",
            "4  i feel suspicious if there is no one outside l...    fear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "EATLR336pUrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`import pandas as pd `This imports the pandas library and gives it the alias pd, which is commonly used to work with data in Python.\n",
        "\n",
        "`df = pd.read_csv('Emotion_classify_Data.csv')` This reads the CSV file named 'Emotion_classify_Data.csv' into a pandas DataFrame (stored in the variable df). The DataFrame allows you to work with the dataset in a structured way, where rows represent individual data points and columns represent the features of the data.\n",
        "\n",
        "`print(df.head())`\n",
        "\n",
        "\n",
        "\n",
        "This prints the first few rows of the DataFrame to help you inspect the data. By default, head() shows the first 5 rows, giving you a quick preview of your dataset's structure."
      ],
      "metadata": {
        "id": "ypyx6DAJplTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Remove rows with missing values (if any)\n",
        "df = df.dropna()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BxDM14elVqG",
        "outputId": "262c31bd-1c2b-4e2c-cc26-6bd02bd6cb03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment    0\n",
            "Emotion    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "zw0CNQzXpyOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`df.isnull().sum()`\n",
        "\n",
        "1.This checks if there are any missing (null) values in the\n",
        "DataFrame df. The isnull() method returns a DataFrame of the same shape as df with True for missing values and False for non-missing values.\n",
        "\n",
        "2.The sum() function then adds up the True values (which are counted as 1), giving you the total number of missing values in each column.\n",
        "\n",
        "`df = df.dropna()`\n",
        "\n",
        "1.This removes all rows from the DataFrame df that contain any missing (NaN) values.\n",
        "2.The dropna() method returns a new DataFrame with the rows containing NaN values dropped, and the result is assigned back to df (replacing the original DataFrame)."
      ],
      "metadata": {
        "id": "SmBWdm1Vp2JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation, numbers, and convert text to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove digits\n",
        "    text = text.lower()                  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to the 'Comment' column\n",
        "df['cleaned_comment'] = df['Comment'].apply(clean_text)\n",
        "\n",
        "# Display the cleaned data\n",
        "print(df[['Comment', 'cleaned_comment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7in5YmZEl5so",
        "outputId": "04c603f1-4740-4001-ae88-d1f8baa35418"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Comment  \\\n",
            "0  i seriously hate one subject to death but now ...   \n",
            "1                 im so full of life i feel appalled   \n",
            "2  i sit here to write i start to dig out my feel...   \n",
            "3  ive been really angry with r and i feel like a...   \n",
            "4  i feel suspicious if there is no one outside l...   \n",
            "\n",
            "                                     cleaned_comment  \n",
            "0  i seriously hate one subject to death but now ...  \n",
            "1                 im so full of life i feel appalled  \n",
            "2  i sit here to write i start to dig out my feel...  \n",
            "3  ive been really angry with r and i feel like a...  \n",
            "4  i feel suspicious if there is no one outside l...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "853EMfxSqNgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.clean_text: This function removes punctuation, digits, and converts text to lowercase.\n",
        "\n",
        "2.apply: Applies the clean_text function to each comment in the DataFrame.\n",
        "\n",
        "3.print: Displays the original and cleaned text for comparison."
      ],
      "metadata": {
        "id": "93f79iX5qVqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the emotion labels\n",
        "df['emotion_label'] = label_encoder.fit_transform(df['Emotion'])\n",
        "\n",
        "# Display the encoded labels\n",
        "print(df[['Emotion', 'emotion_label']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcyjCrNel-5_",
        "outputId": "e61876e9-899a-4db6-f0b6-2529a07639e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Emotion  emotion_label\n",
            "0    fear              1\n",
            "1   anger              0\n",
            "2    fear              1\n",
            "3     joy              2\n",
            "4    fear              1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "SChG-Uupqlfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.LabelEncoder(): This initializes the label encoder, which converts categorical text labels (e.g., emotions) into numeric values.\n",
        "\n",
        "2.fit_transform(): This method encodes the 'Emotion' column into numeric labels and stores them in a new column 'emotion_label'.\n",
        "\n",
        "3.print: Displays the original emotion labels alongside their encoded numeric labels for reference."
      ],
      "metadata": {
        "id": "gY6ubtQmqqAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = df['cleaned_comment']  # Features: The cleaned comments (text)\n",
        "y = df['emotion_label']    # Target: The corresponding emotions\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of training and test sets\n",
        "print(len(X_train), len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_zoaICUmEpW",
        "outputId": "787e13cd-6baf-40ee-8191-b914eada008e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4749 1188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "rXL4B2XZq6wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  train_test_split: Splits the dataset into training (80%) and testing (20%) sets.\n",
        "\n",
        "*  X: The feature (cleaned text comments).\n",
        "*   y: The target (emotion labels).\n",
        "\n",
        "\n",
        "*   print: Displays the size of the training and testing sets.\n",
        "\n"
      ],
      "metadata": {
        "id": "7oixAoqzrCnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data (without fitting again)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Check the shape of the transformed data\n",
        "print(X_train_tfidf.shape, X_test_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-XV3FWnmJOf",
        "outputId": "5f4e55f0-0882-4dd7-c524-94d012c5fff4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4749, 5000) (1188, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n",
        "\n"
      ],
      "metadata": {
        "id": "hf-q3PAqre2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.TfidfVectorizer: Converts the text data into numerical vectors based on term frequency-inverse document frequency (TF-IDF), which helps capture important words in the text.\n",
        "\n",
        "2.fit_transform(X_train): Fits the vectorizer to the training data and transforms it into numerical features.\n",
        "\n",
        "3.transform(X_test): Transforms the test data into numerical features using the already fitted vectorizer.\n",
        "\n",
        "4.print: Displays the shape of the transformed data, showing the number of samples and features."
      ],
      "metadata": {
        "id": "uCHNguNarjHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrfJuTNtmNuu",
        "outputId": "1bd7d56c-1d03-4b41-d753-29c54dc190cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.92      0.92       392\n",
            "           1       0.95      0.90      0.93       416\n",
            "           2       0.90      0.94      0.92       380\n",
            "\n",
            "    accuracy                           0.92      1188\n",
            "   macro avg       0.92      0.92      0.92      1188\n",
            "weighted avg       0.92      0.92      0.92      1188\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "OsnJ3TV-rq_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.LogisticRegression(): Initializes a logistic regression model, which is used for classification tasks.\n",
        "\n",
        "2.fit(X_train_tfidf, y_train): Trains the model on the transformed training data (X_train_tfidf) and the corresponding labels (y_train).\n",
        "\n",
        "3.predict(X_test_tfidf): Uses the trained model to predict the emotions for the test set (X_test_tfidf).\n",
        "\n",
        "4.classification_report: Evaluates the model's performance, providing metrics like precision, recall, and F1-score for each emotion label."
      ],
      "metadata": {
        "id": "HwNRFce7ruee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define responses for each emotion\n",
        "emotion_responses = {\n",
        "    'fear': 'I’m here for you. It’s okay to feel scared sometimes.',\n",
        "    'anger': 'I understand your frustration. Let’s try to talk it out.',\n",
        "    'joy': 'That’s wonderful to hear! Keep the positivity flowing!',\n",
        "    'sadness': 'I’m really sorry you’re feeling this way. I’m here to help.',\n",
        "    # Add more emotions and responses as needed\n",
        "}\n",
        "\n",
        "# Function to generate a response based on emotion\n",
        "def generate_response(emotion):\n",
        "    return emotion_responses.get(emotion, \"I’m here to listen.\")\n",
        "\n",
        "# Predict emotion and generate response for the first test sample\n",
        "predicted_emotion = label_encoder.inverse_transform([y_pred[0]])  # Get the predicted emotion\n",
        "response = generate_response(predicted_emotion[0])\n",
        "\n",
        "# Print the response\n",
        "print(\"Predicted Emotion:\", predicted_emotion[0])\n",
        "print(\"Response:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5KfEdLFmRK0",
        "outputId": "877c5035-0fa8-45f0-8c38-8d1559867e9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: anger\n",
            "Response: I understand your frustration. Let’s try to talk it out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLANATION\n"
      ],
      "metadata": {
        "id": "LDgmUe5Ir74k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.emotion_responses: A dictionary mapping emotions to predefined empathetic responses.\n",
        "\n",
        "2.generate_response(emotion): A function that returns an appropriate response based on the predicted emotion.\n",
        "\n",
        "3.inverse_transform([y_pred[0]]): Converts the predicted numeric emotion back to the original text label.\n",
        "\n",
        "4.print: Displays the predicted emotion and the corresponding response."
      ],
      "metadata": {
        "id": "f4YOA_4Cr_c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(user_input):\n",
        "    # Clean the user input\n",
        "    cleaned_input = clean_text(user_input)\n",
        "\n",
        "    # Transform the input using TF-IDF\n",
        "    user_input_tfidf = tfidf_vectorizer.transform([cleaned_input])\n",
        "\n",
        "    # Predict the emotion\n",
        "    predicted_label = model.predict(user_input_tfidf)\n",
        "    predicted_emotion = label_encoder.inverse_transform(predicted_label)\n",
        "\n",
        "    # Generate the response\n",
        "    response = generate_response(predicted_emotion[0])\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example usage:\n",
        "user_input = \"I'm feeling really anxious today\"\n",
        "print(\"Bot:\", chat_with_bot(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLnO-UTmmzjM",
        "outputId": "fa22814f-f403-4a7e-80dd-52a3f6adc9af"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: I’m here for you. It’s okay to feel scared sometimes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY\n"
      ],
      "metadata": {
        "id": "BN-rWpznsTkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project builds a chatbot that detects a user's emotional state from text and responds empathetically. It involves:\n",
        "\n",
        "1.Data Preprocessing: Cleaning and encoding text data labeled with emotions like fear, anger, and joy.\n",
        "\n",
        "2.Model Training: Using a Logistic Regression model to classify emotions based on text.\n",
        "\n",
        "3.Feature Extraction: Transforming text into numerical features with TF-IDF.\n",
        "\n",
        "4.Response Generation: Providing culturally relevant, empathetic responses based on the predicted emotion.\n",
        "\n",
        "The goal is to create a chatbot that understands emotional cues and offers supportive, context-aware replies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PEh_yovOsX8t"
      }
    }
  ]
}